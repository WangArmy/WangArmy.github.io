<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>用MPI进行分布式内存编程</title>
    <url>/2020/07/25/MPI/</url>
    <content><![CDATA[<h1><center>前言</center></h1>
<p>  <strong>分布式内存系统</strong>，顾名思义，是由多个处理器(CPU)组成，每个处理器可以位于不同的计算机上，并且都有各自私有的内存。<br>
  <strong>MPI</strong>(Message-Passing Interface)，是运行在分布式计算机系统上的并行应用程序所使用的最流行的通信协议。它不是一种新的语言，它定义了可以被C、C++和Fortran程序调用的函数库。<br>
  在消息传递程序<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>中， 运行在一个 核 - 内存 对上的程序统称为一个进程。两个进程可以通过调用函数来进行通信：一个进程调用发送函数，一个进程调用接受函数，该通信方式的实现即为MPI。</p>
<a id="more"></a>
<h1><center>基础</center></h1>
<h2 id="编译执行"><a class="header-anchor" href="#编译执行">¶</a>编译执行</h2>
<h3 id="使用mpicc命令来编译程序"><a class="header-anchor" href="#使用mpicc命令来编译程序">¶</a>使用mpicc命令来编译程序<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup></h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ mpicc -g -Wall -o mpi_exec mpi_exec.c</span><br></pre></td></tr></table></figure>
<ul>
<li>mpicc是C语言编译器的wrapper script。</li>
<li><code>$</code>：shell提示符</li>
<li><code>-g</code>：需要对程序进行gdb调试则加上该选项</li>
<li><code>-Wall</code>：产生警告信息</li>
<li><code>-o</code>：后接产生的可执行文件的名称</li>
</ul>
<h3 id="使用mpiexec命令启动程序"><a class="header-anchor" href="#使用mpiexec命令启动程序">¶</a>使用mpiexec命令启动程序</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ mpiexec -n &lt;number of processes&gt; ./mpi_exec</span><br></pre></td></tr></table></figure>
<ul>
<li><code>-n &lt;number of processes&gt;</code>：告诉系统启动&lt;number of processes &gt;个&lt;mpi_exec&gt;程序的实例，进程运行后，MPI保证进程间可以相互通信。</li>
</ul>
<h2 id="基本函数"><a class="header-anchor" href="#基本函数">¶</a>基本函数</h2>
<h3 id="MPI-Init和MPI-Finalize"><a class="header-anchor" href="#MPI-Init和MPI-Finalize">¶</a>MPI_Init和MPI_Finalize</h3>
<h4 id="MPI-Init"><a class="header-anchor" href="#MPI-Init">¶</a>MPI_Init()</h4>
<ul>
<li>告知MPI系统进行所有必要的初始化设置，即并行环境初始化</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Init</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span>* argc_p         <span class="comment">/* in/out */</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">char</span>*** argv_p      <span class="comment">/* in/out */</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure>
<ol>
<li>参数 $argc_p$ 和 $argv_p$ 是指向 $main$ 主函数的参数 $argc$ 和 $argv$ 的指针。(不使用这些参数时，设置为 $NULL$ 即可)</li>
<li><code>MPI_Init</code> 函数返回一个 $int$ 型的错误码，通常情况下我们忽略这些错误码。</li>
<li>在程序调用 <code>MPI_Init</code> 之前，不应该调用其它MPI函数。(除 <code>MPI_Initialized</code> 函数<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>)</li>
</ol>
<h4 id="MPI-Finalize"><a class="header-anchor" href="#MPI-Finalize">¶</a>MPI_Finalize</h4>
<ul>
<li>告知MPI系统MPI已经使用完毕，为MPI分配的资源可以释放</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Finalize</span><span class="params">(<span class="keyword">void</span>)</span></span>;</span><br></pre></td></tr></table></figure>
<ol>
<li>退出MPI系统，所有进程正常退出都必须调用该函数。</li>
<li>串行代码仍可在主进程上运行，但是不可以再调用任何MPI函数。</li>
</ol>
<h4 id="标准MPI程序框架"><a class="header-anchor" href="#标准MPI程序框架">¶</a>标准MPI程序框架</h4>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;mpi.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span>&#123;</span><br><span class="line">    ......</span><br><span class="line">    <span class="comment">/* No MPI calls before this */</span></span><br><span class="line">    MPI_Init(&amp;argc, &amp;argv);</span><br><span class="line">    ......</span><br><span class="line">    MPI_Finalize();</span><br><span class="line">    <span class="comment">/*  No MPI calls after this */</span></span><br><span class="line">    ......</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<h3 id="MPI-Comm-size和MPI-Comm-rank"><a class="header-anchor" href="#MPI-Comm-size和MPI-Comm-rank">¶</a>MPI_Comm_size和MPI_Comm_rank</h3>
<h4 id="通信子"><a class="header-anchor" href="#通信子">¶</a>通信子</h4>
<ul>
<li>MPI中， communicator指一组可以互相发送消息的进程集合。</li>
<li><code>MPI_Init</code>的一个初始化设置即：<br>
在用户启动程序时，定义由用户启动的所有进程所组成的通信子，该通信子为 : <code>MPI_COMM_WORLD</code>。</li>
<li>MPI为通信子定义了一种特殊的类型：<code>MPI_Comm</code></li>
</ul>
<h4 id="MPI-Comm-size"><a class="header-anchor" href="#MPI-Comm-size">¶</a>MPI_Comm_size</h4>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Comm_size</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Comm comm      <span class="comment">/* in  */</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span>* comm_sz_p     <span class="comment">/* out */</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>获得通信子<code>comm</code>中的进程数，并存在<code>comm_sz_p</code>中。</li>
</ul>
<h4 id="MPI-Comm-rank"><a class="header-anchor" href="#MPI-Comm-rank">¶</a>MPI_Comm_rank</h4>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Comm_size</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Comm comm      <span class="comment">/* in  */</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span>* my_rank_p     <span class="comment">/* out */</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>获取正在调用的进程在通信子中的进程号，并存在<code>my_rank_p</code>中。</li>
</ul>
<h3 id="MPI-Send和MPI-Recv"><a class="header-anchor" href="#MPI-Send和MPI-Recv">¶</a>MPI_Send和MPI_Recv</h3>
<h4 id="SPMD程序"><a class="header-anchor" href="#SPMD程序">¶</a>SPMD程序</h4>
<p>  通常来说，MPI程序会运行多个进程。为了实现并行程序，我们会编写单个MPI程序，让不同进程产生不同的动作。让进程按照它们的进程号来匹配程序分支，该方法称为单程序多数据流(Simple Program Multiple Data)。</p>
<ul>
<li>下面的程序即通过使用 if-else 语句实现SPMD。</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">* File name: hello.c</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;    /* For strlen */</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;mpi.h&gt;       /* For MPI functions, etc */</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_STRING 100</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">char</span> greeting[MAX_STRING];</span><br><span class="line">    <span class="keyword">int</span> comm_sz;    <span class="comment">/* Number of processes */</span></span><br><span class="line">    <span class="keyword">int</span> my_rank;    <span class="comment">/* My process rank */</span></span><br><span class="line">    </span><br><span class="line">    MPI_Init(<span class="literal">NULL</span>, <span class="literal">NULL</span>);</span><br><span class="line">    MPI_Comm_size(MPI_COMM_WORLD, &amp;comm_sz);</span><br><span class="line">    MPI_Comm_rank(MPI_COMM_WORLD, &amp;my_rank);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(my_rank != <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="built_in">sprintf</span>(greeting, <span class="string">"Greetings from process %d of %d!"</span>, my_rank, comm_sz);</span><br><span class="line">        MPI_Send(greeting, <span class="built_in">strlen</span>(greeting)+<span class="number">1</span>, MPI_CHAR, <span class="number">0</span>, <span class="number">0</span>, MPI_COMM_WORLD);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Greetings from process %d of %d!\n"</span>, my_rank, comm_sz);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> q=<span class="number">1</span>; q&lt;comm_sz; q++)&#123;</span><br><span class="line">            MPI_Recv(greeting, MAX_STRING, MPI_CHAR, q, <span class="number">0</span>, MPI_COMM_WORLD, MPI_STATUS_IGNORE);</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"%s\n"</span>, greeting);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    MPI_Finalize();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;   <span class="comment">/* main */</span></span><br></pre></td></tr></table></figure>
<ul>
<li>编译运行</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mpicc -g -Wall -o hello hello.c</span><br><span class="line">mpiexec -n 4 .&#x2F;hello</span><br></pre></td></tr></table></figure>
<ul>
<li>运行结果<br>
<img src="hello.png" alt=""></li>
</ul>
<h4 id="通信"><a class="header-anchor" href="#通信">¶</a>通信</h4>
<p>  在上述程序中，我们可以发现除了0号进程外，其余的进程都使用<code>sprintf</code>函数生成了一条要发送给0号进程的消息，并使用<code>MPI_Send</code>将消息发送给0号进程。同时，0号进程也使用<code>MPI_Recv</code>接受消息，并用<code>printf</code>函数将消息打印出。<br>
  0号进程使用for循环依次接受来自1~comm_sz-1 号进程发送来的消息。</p>
<h4 id="span-id-jump2-MPI-Send-span"><a class="header-anchor" href="#span-id-jump2-MPI-Send-span">¶</a><span id="jump2">MPI_Send</span></h4>
<ul>
<li>进程间发送消息</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Send</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">void</span>*         msg_buf_p      <span class="comment">/* in */</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span>           msg_size       <span class="comment">/* in */</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Datatype  msg_type       <span class="comment">/* in */</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span>           dest           <span class="comment">/* in */</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span>           tag            <span class="comment">/* in */</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Comm      communicator   <span class="comment">/* in */</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure>
<p> 参数解析<br>
<code>msg_buf_p</code>：指向包含发送消息内容的内存块的指针<br>
<code>msg_size</code>：发送的消息的个数<br>
<code>msg_type</code>：发送消息的数据类型<br>
<code>dest</code>：目的进程号<br>
<code>tag</code>：消息标签，接受方需要有相同标签才能接受消息<br>
<code>communicator</code>：通信子，表示在该通信组内传输消息</p>
<ul>
<li>由于C语言中的数据类型不能作为参数传递给函数，MPI定义了一个特殊的类型：<a href="#jump1">MPI_Datatype</a>，用于参数msg_type。</li>
<li><code>msg_size</code>和<code>msh_type</code>指定了发送的数据量，对于<code>MPI_CHAR</code>，因为C语言中字符串结束符为<code>'\0'</code>，所以<code>msg_size</code>为<code>strlen(greeting)+1</code>。</li>
</ul>
<h4 id="MPI-Recv"><a class="header-anchor" href="#MPI-Recv">¶</a>MPI_Recv</h4>
<ul>
<li>接受消息</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Recv</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">void</span>*         msg_buf_p     <span class="comment">/* out */</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span>           msg_size      <span class="comment">/* in  */</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Datatype  msg_type      <span class="comment">/* in  */</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span>           source        <span class="comment">/* in  */</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span>           tag           <span class="comment">/* in  */</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Comm      communicator  <span class="comment">/* in  */</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Status*   status_p      <span class="comment">/* out */</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure>
<p> 参数解析<br>
<code>msg_buf_p</code>：指向接受信息的内存块<br>
<code>msg_size</code>：指定内存中要存储对象的个数<br>
<code>msg_type</code>：待存储对象的类型<br>
<code>source</code>：接收的消息的源进程号<br>
<code>tag</code>：消息标签，和发送方的tag值相同时才能接受该消息<br>
<code>communicator</code>：通信子，表示在该通信组内传输消息<br>
<code>status_p</code>：消息状态。接受函数返回后，将在这个参数指示的变量中存储实际接受消息的状态，包括<code>MPI_SOURCE</code>、<code>MPI_TAG</code>、<code>MPI_ERROR</code></p>
<ul>
<li>接受到的数据量可以通过函数<code>MPI_Get_count</code>得到</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Get_count</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Status*    status_p <span class="comment">/* in  */</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    MPI_Datatype   type     <span class="comment">/* in  */</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span>*           count_p  <span class="comment">/* out */</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span>;</span><br></pre></td></tr></table></figure>
<h4 id="关于MPI-Send和MPI-Recv"><a class="header-anchor" href="#关于MPI-Send和MPI-Recv">¶</a>关于MPI_Send和MPI_Recv</h4>
<ul>
<li>在<code>MPI_Send</code>发送一条消息后，该消息会被放入到缓冲区中，此时系统会出现阻塞，只有当缓冲区可以重新使用时，才会返回<code>MPI_Send</code>的调用。 当消息被<strong>发送至匹配的接受缓冲区</strong>或者<strong>被拷贝至一个中间系统缓冲区</strong>时，发送数据的缓冲区会被释放。</li>
<li>对于<code>MPI_Recv</code>，在接受消息的时候，系统会产生阻塞，直到接受到一条匹配的消息。 如果一个进程试图接受消息，但是没有匹配的消息，那么进程就会被永远阻塞在那里，即<strong>进程悬挂</strong>。</li>
<li>MPI要求消息是<strong>不可超越的</strong>。进程发送多条消息给其它进程时，要确保发送消息的现后顺序，只有当前消息发送成功后才能继续发送下一消息。</li>
<li>如果<code>MPI_Send</code>发生阻塞，且没有相匹配的接受，那么发送进程会悬挂起来；若被缓冲，但没有匹配的接受，那么消息将丢失。</li>
</ul>
<h2 id="补充"><a class="header-anchor" href="#补充">¶</a>补充</h2>
<h3 id="span-id-jump1-MPI-Datatype-span"><a class="header-anchor" href="#span-id-jump1-MPI-Datatype-span">¶</a><span id="jump1"><a href="#jump2">MPI_Datatype</a></span></h3>
<table>
<thead>
<tr>
<th style="text-align:center">MPI Datatype</th>
<th style="text-align:center">C Datatype</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">MPI_CHAR</td>
<td style="text-align:center">signed char</td>
</tr>
<tr>
<td style="text-align:center">MPI_SHORT</td>
<td style="text-align:center">signed short int</td>
</tr>
<tr>
<td style="text-align:center">MPI_INT</td>
<td style="text-align:center">signed int</td>
</tr>
<tr>
<td style="text-align:center">MPI_LONG</td>
<td style="text-align:center">signed long int</td>
</tr>
<tr>
<td style="text-align:center">MPI_LONG_LONG</td>
<td style="text-align:center">signed long long int</td>
</tr>
<tr>
<td style="text-align:center">MPI_UNSIGNED_CHAR</td>
<td style="text-align:center">unsigned char</td>
</tr>
<tr>
<td style="text-align:center">MPI_UNSIGNED_SHORT</td>
<td style="text-align:center">unsigned short int</td>
</tr>
<tr>
<td style="text-align:center">MPI_UNSIGNED</td>
<td style="text-align:center">unsigned int</td>
</tr>
<tr>
<td style="text-align:center">MPI_UNSIGNED_LONG</td>
<td style="text-align:center">unsigned long int</td>
</tr>
<tr>
<td style="text-align:center">MPI_FLOAT</td>
<td style="text-align:center">float</td>
</tr>
<tr>
<td style="text-align:center">MPI_DOUBLE</td>
<td style="text-align:center">double</td>
</tr>
<tr>
<td style="text-align:center">MPI_LONG_DOUBLE</td>
<td style="text-align:center">long double</td>
</tr>
<tr>
<td style="text-align:center">MPI_BYTE</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">MPI_PACKED</td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<h1><center>进阶</center></h1>
<h2 id="用MPI来实现梯形积分法"><a class="header-anchor" href="#用MPI来实现梯形积分法">¶</a>用MPI来实现梯形积分法</h2>
<h3 id="梯形积分法"><a class="header-anchor" href="#梯形积分法">¶</a>梯形积分法</h3>
<p>我们将通过编写程序来实现数值积分中的梯形积分法<br>
<img src="trapezoidal_rule.png" alt=""><br>
  如上图所示，梯形积分法可以估计函数$y=f(x)$的图像中，两条垂直线与$x$轴之间的区域大小。我们将x轴上的区间划分为$n$个等长子区间，然后估计介于函数图像及每个子区间内的梯形区域的面积。<br>
  那么梯形区域的面积为:<br>
$$Area\ of\ one\ trapezoid = \frac{h}{2}[f(x_i)+f(x_i+1)] $$<br>
  每个子区间的长度为:<br>
$$h\ =\ \frac{b-a}{n}$$<br>
  我们划分的子区间的端点为:<br>
$$x_0=a,\ x_1=a+h,\ x_2=a+2h,\ \dots,\ x_{n-1}=a+(n-1)h,\ x_n=b$$<br>
  梯形面积和为:<br>
$$Sum\ of\ trapezoid\ areas = h[f(x_0)/2+f(x_1)+\dots+f(x_{n-1})+f(x_n)/2]$$<br>
  综上，串行程序伪代码如下所示:</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* Input: a, b, n */</span></span><br><span class="line">h = (b-a)/n;</span><br><span class="line">approx = (f(a) + f(b))/<span class="number">2.0</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;n; i++)&#123;</span><br><span class="line">    x_i = a + i*h;</span><br><span class="line">    approx+=f(x_i);</span><br><span class="line">&#125;</span><br><span class="line">approx = h*approx;</span><br></pre></td></tr></table></figure>
<h3 id="并行化梯形积分法"><a class="header-anchor" href="#并行化梯形积分法">¶</a>并行化梯形积分法</h3>
<p>  我们将会使用下列四个基本步骤去设计一个并行程序：<br>
 1. Partition problem solution into tasks<br>
 2. Identify communication channels between tasks<br>
 3. Aggregate tasks into composite tasks<br>
 4. Map composite tasks to cores</p>
<p>  在任务划分阶段，我们会将梯形积分法划分为两个任务</p>
<ul>
<li>获取单个矩形区域面积</li>
<li>计算这些区域的面积和</li>
</ul>
<p>  我们假设将区间 $[a,b]$ 分成comm_sz个子区间。</p>
<p>  并行程序伪代码如下所示：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">* 全局变量，在所有进程内有效</span></span><br><span class="line"><span class="comment">* n: 划分的区间总数</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">Get a, b, n;</span><br><span class="line">h = (b-a)/n;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">* 每一个子进程都会有各自独立的内存空间</span></span><br><span class="line"><span class="comment">* 下列的变量为局部变量，在每个子进程内都会有实例，只在其进程内有效</span></span><br><span class="line"><span class="comment">* comm_sz：并行程序会产生comm_sz个进程来执行该计算任务</span></span><br><span class="line"><span class="comment">* my_rank: 正在调用的进程的进程号</span></span><br><span class="line"><span class="comment">* local_n: 当前进程的区间号，用来指示区间的上下界local_a和local_b</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">local_n = n/comm_sz;</span><br><span class="line">local_a = a + my_rank*local_n*h;</span><br><span class="line">local_b = local_a + local_n*h;</span><br><span class="line">local_integral = Trap(local_a, local_b, local_n, h);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (my_rank != <span class="number">0</span>) &#123;</span><br><span class="line">    Send local_integral to <span class="built_in">process</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123; <span class="comment">/* my_rank==0 */</span></span><br><span class="line">    total_integral = local_integral;</span><br><span class="line">    <span class="keyword">for</span> (proc=<span class="number">1</span>; proc &lt; comm_sz; proc++) &#123;</span><br><span class="line">        Receive local_integral from proc;</span><br><span class="line">        total_integral += local_integral;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (my_rank == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="built_in">print</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="具体实现"><a class="header-anchor" href="#具体实现">¶</a>具体实现</h3>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">* File name: Trapezoidal_Rule.c</span></span><br><span class="line"><span class="comment">* Decrisption: 计算函数f(x) = x^2在区间[0,3]的积分</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;mpi.h&gt; /* For MPI functions, etc */</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义函数</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">f</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">double</span> pos         <span class="comment">/* in */</span>)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> pos*pos;</span><br><span class="line">&#125;   <span class="comment">/* f */</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">Trap</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">double</span> left_endpt  <span class="comment">/* in */</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">double</span> right_endpt <span class="comment">/* in */</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">int</span>    trap_count  <span class="comment">/* in */</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">double</span> base_len    <span class="comment">/* in */</span>)</span></span>&#123;</span><br><span class="line">    <span class="keyword">double</span> estimate, x;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    estimate = (f(left_endpt) + f(right_endpt))/<span class="number">2.0</span>;</span><br><span class="line">    <span class="keyword">for</span> (i=<span class="number">1</span>; i&lt;trap_count; i++) &#123;</span><br><span class="line">        x = left_endpt + i*base_len;</span><br><span class="line">        estimate += f(x);</span><br><span class="line">    &#125;</span><br><span class="line">    estimate = estimate*base_len;</span><br><span class="line">    <span class="keyword">return</span> estimate;</span><br><span class="line">&#125;   <span class="comment">/* Trap */</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> my_rank, comm_sz, n = <span class="number">1024</span>, local_n;</span><br><span class="line">    <span class="keyword">double</span> a = <span class="number">0.0</span>, b = <span class="number">3.0</span>, h, local_a, local_b;</span><br><span class="line">    <span class="keyword">double</span> local_int, total_int;</span><br><span class="line">    <span class="keyword">int</span> source;</span><br><span class="line"></span><br><span class="line">    MPI_Init(<span class="literal">NULL</span>, <span class="literal">NULL</span>);</span><br><span class="line">    MPI_Comm_rank(MPI_COMM_WORLD, &amp;my_rank);</span><br><span class="line">    MPI_Comm_size(MPI_COMM_WORLD, &amp;comm_sz);</span><br><span class="line"></span><br><span class="line">    h = (b-a)/n;          <span class="comment">/* h is the same for all processes */</span></span><br><span class="line">    local_n = n/comm_sz;  <span class="comment">/* local_n is the number of trapezoids */</span></span><br><span class="line"></span><br><span class="line">    local_a = a + my_rank*local_n*h;</span><br><span class="line">    local_b = local_a + local_n*h;</span><br><span class="line">    local_int = Trap(local_a, local_b, local_n, h);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (my_rank != <span class="number">0</span>) &#123;</span><br><span class="line">        MPI_Send(&amp;local_int, <span class="number">1</span>, MPI_DOUBLE, <span class="number">0</span>, <span class="number">0</span>, MPI_COMM_WORLD);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        total_int = local_int;</span><br><span class="line">        <span class="keyword">for</span> (source=<span class="number">1</span>; source&lt;comm_sz; source++) &#123;</span><br><span class="line">            MPI_Recv(&amp;local_int, <span class="number">1</span>, MPI_DOUBLE, source, <span class="number">0</span>, MPI_COMM_WORLD, MPI_STATUS_IGNORE);</span><br><span class="line">            total_int += local_int;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (my_rank == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"With n = %d trapezoids, our estimate\n"</span>, n);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"of the integral from %f to %f = %.15e\n"</span>, a, b, total_int);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    MPI_Finalize();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;   <span class="comment">/* main */</span></span><br><span class="line">``` </span><br><span class="line">+ 编译运行</span><br></pre></td></tr></table></figure>
<p>mpicc -g -Wall -o Trapezoidal_Rule Trapezoidal_Rule.c<br>
mpiexec -n 8 Trapezoidal_Rule</p>
<pre><code>+ 运行结果
![](result.png)</code></pre>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>使用消息传递来对分布式内存系统进行编程所得到的程序。 <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>我们假定使用GNU C编译器。 <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p><code>int MPI_Initialized(int* flag)</code>：用来检测MPI系统是否已经初始化，若已调用 <code>MPI_Init</code> 或 <code>MPI_Init_thread</code>，flag置为 true, 否则置为 false。 <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
]]></content>
      <categories>
        <category>并行计算</category>
      </categories>
      <tags>
        <tag>MPI</tag>
        <tag>函数</tag>
      </tags>
  </entry>
</search>
